install.packages('ggplot2', 'reshape2', 'dplyr', 'plyr', 'MASS', 'igraph')
install.packages('ggplot2')
install.packages('reshape2', "")
install.packages('reshape2')
install.packages("reshape2")
install.packages('plyr')
install.packages('dplyr')
setRepositories()
ap <- available.packages()
ap
View(ap)
"dplyr" %in% rownames(ap)
"MASS" %in% rownames(ap)
library(installr)
install.packages(installr)
install.packages('installr')
updateR()
gp
source('~/Documentos/PhD/src/neighborhood_motifs/sandbox/illustration_breakpoints.r')
source('~/Documentos/PhD/src/neighborhood_motifs/sandbox/illustration_order_neighbourhood.r')
setwd("~/Documentos/PhD/src/Thread growth models")
sourde("trees")
sourde("trees.Rda")
load("trees.Rda")
set.seed(1)
alphas <- c(0.1,0.5)
betas <- c(0.1,0.5)
taus <- c(0.1, 0.5)
z <- c(1,2)
source('~/Documentos/PhD/src/Thread growth models/thread_generator.r')
library(parallel)
cl <- makeCluster(detectCores()-1)
clusterEvalQ(cl, {library(igraph); source('thread_generator.r')})
clusterExport(cl, c("alphas", "betas", "taus", "z"))
trees <- parLapply(cl, 1:1000, function(i) gen.thread.Lumbreras2016(n=100, z=z, alphas=alphas, betas=betas, taus=taus) )
stopCluster(cl)
save(trees, file='trees.Rda')
load('trees.Rda')
# Prepare all the information needed into a nice dataframe
library(parallel)
ncores <- detectCores() - 2
cl<-makeCluster(ncores, outfile="", port=11439)
clusterEvalQ(cl, {library(igraph); library(dplyr)})
data.parlapply <- parLapply(cl, trees, tree.to.data)
stopCluster(cl)
# join results adding thread id
for (i in 1:length(data.parlapply)){
data.parlapply[[i]]$thread <- i
}
data.parlapply <- rbindlist(data.parlapply)
df.trees <- data.frame(thread = data.parlapply$thread,
user = data.parlapply$user,
post = data.parlapply$post,
t = data.parlapply$t,
parent = data.parlapply$parent,
popularity = data.parlapply$popularity,
lag = data.parlapply$lag)
library(dplyr)
library(data.table)
library(igraph)
source('~/Documentos/PhD/src/Thread growth models/likelihood.r')
# Prepare all the information needed into a nice dataframe
library(parallel)
ncores <- detectCores() - 2
cl<-makeCluster(ncores, outfile="", port=11439)
clusterEvalQ(cl, {library(igraph); library(dplyr)})
data.parlapply <- parLapply(cl, trees, tree.to.data)
stopCluster(cl)
# join results adding thread id
for (i in 1:length(data.parlapply)){
data.parlapply[[i]]$thread <- i
}
data.parlapply <- rbindlist(data.parlapply)
df.trees <- data.frame(thread = data.parlapply$thread,
user = data.parlapply$user,
post = data.parlapply$post,
t = data.parlapply$t,
parent = data.parlapply$parent,
popularity = data.parlapply$popularity,
lag = data.parlapply$lag)
df.trees
alphas
betas
z
taus
df.trees <- filter(df.trees, t>2)
K <- length(alphas)
users <- unique(df.trees$user)
U <- length(users)
responsabilities <- matrix(nrow = U, ncol = K)
pis <- rep(1/ncol(responsabilities), ncol(responsabilities))
responsabilities
for (u in 1:U){
responsabilities[u,] <- update_responsabilities(df.trees, u, pis, alphas, betas, taus)
}
update_responsabilities <- function(df.trees, u, pis, alphas, betas, taus){
# Compute E(z_uk) over the posterior distribution p(z_uk | X, theta)
K <- length(alphas) # number of clusters
Xu <- filter(df.trees, user==u) # all posts from user
logfactors <- rep(0,K)
for (k in 1:K){
logfactors[k] <- log(pis[k]) + sum(apply(Xu, 1, likelihood.post, alphas[k], betas[k], taus[k]))
}
logfactors <- logfactors - max(logfactors) # avoid numerical underflow
denominator <- sum(exp(logfactors))
responsabilities_u <- exp(logfactors)/denominator
responsabilities_u
}
for (u in 1:U){
responsabilities[u,] <- update_responsabilities(df.trees, u, pis, alphas, betas, taus)
}
responsabilities
k <- 1
alphas
responsabilities_k <- responsabilities[,k]
pis_k <- pis[k]
opt.grid <- function(Q, params0, ...){
# Grid search
registerDoParallel(cores = 5)
df.results <- foreach(alpha = seq(0.1,1, by=0.2), .combine = rbind) %dopar% {
foreach(beta = seq(0.1,1, by=0.2), .combine = rbind) %dopar% {
foreach(tau = seq(0.1,3, by=0.2), .combine = rbind) %dopar% {
like <- - Q(alpha, beta, tau, ...)
data.frame(alpha = alpha, beta = beta, like = like)
}
df.results[which.max(df.results$like),]
}
opt.grid <- function(Q, params0, ...){
# Grid search
registerDoParallel(cores = 5)
df.results <- foreach(alpha = seq(0.1,1, by=0.2), .combine = rbind) %dopar% {
foreach(beta = seq(0.1,1, by=0.2), .combine = rbind) %dopar% {
foreach(tau = seq(0.1,3, by=0.2), .combine = rbind) %dopar% {
like <- - Q(alpha, beta, tau, ...)
data.frame(alpha = alpha, beta = beta, like = like)
}
df.results#[which.max(df.results$like),]
}
library(doParallel)
library(foreach)
pi_k
pis_k
df.results <- opt.grid(Qopt, params0=c(1,2,3),
resp_k = responsabilities_k,
pi_k = pis_k,
df.trees = df.trees)
Qopt <- function(alpha, beta, tau, resp_k, pi_k, df.trees){
# sum of E[lnp(X,Z|\theta)] likelihoods for all clusters and all users
# given the current responsabilities
# Note that the optimizations can be done separatedly
a <- resp_k[df.trees$user]
b <- apply(df.trees, 1, function(x) likelihood.post(x, alpha, beta, tau) + log(pi_k))
sum(a*b)
}
df.results <- opt.grid(Qopt, params0=c(1,2,3),
resp_k = responsabilities_k,
pi_k = pis_k,
df.trees = df.trees)
