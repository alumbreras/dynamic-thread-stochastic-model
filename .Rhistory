install.packages("igraph")
install.packages("igraph")
install.packages("igraph")
install.packages("igraph")
install.packages("igraph")
install.packages("poweRlaw")
install.packages(c('rzmq','repr','IRkernel','IRdisplay'),
repos = c('http://irkernel.github.io/', getOption('repos')))
IRkernel::installspec()
install.packages("devtools")
install.packages("httr")
install.packages("curl")
install.packages('RCurl')
install.packages('RCurl')
install.packages('RCurl')
install.packages('RCurl')
install.packages("devtools")
install.packages('RCurl')
install_github('armstrtw/rzmq')
library(devtools)
install_github('armstrtw/rzmq')
install_github("takluyver/IRdisplay")
install_github("takluyver/IRkernel")
IRkernel::installspec()
install_github("takluyver/IRkernel")
load("~/Documents/PhD/Graphs/R/speed_of_structures_100k.RData")
par(mfrow=c(1,1))
x.offset <- ifelse(df$type=="star", -0.2, 0.2)
plot(df$size+x.offset, df$mean, col=colors, xlab="size", ylab="mean response time (seconds)")
legend("topright", c("star", "chain"), col = c("blue", "red"), pch=1)
title("speed of of starts and chains")
source('~/Documents/PhD/Graphs/R/speed_of_structures.r')
# Mixture of two Gaussians
z <- runif(nsamples)<0.5 # assume mixture coefficients are (0.5,0.5)
x1_x2 <- rnorm(nsamples,mean=ifelse(z,-10,10),sd=1)
hist(x1_x2,breaks=100)
par(mfrow=c(2,1))
nsamples <- 100000
# Sum of two Gaussians
x1 <- rnorm(nsamples, mean=-10, sd=1)
x2 <- rnorm(nsamples, mean=10, sd=1)
hist(x1+x2, breaks=100)
# Mixture of two Gaussians
z <- runif(nsamples)<0.5 # assume mixture coefficients are (0.5,0.5)
x1_x2 <- rnorm(nsamples,mean=ifelse(z,-10,10),sd=1)
hist(x1_x2,breaks=100)
par(mfrow=c(1,1))
hist(x1+x2, breaks=100)
hist(x1_x2,breaks=100)
x1_x2
points(x1_x2)
?points
require(stats) # for rnorm
plot(-4:4, -4:4, type = "n")  # setting up coord. system
points(rnorm(200), rnorm(200), col = "red")
points(rnorm(100)/2, rnorm(100)/2, col = "blue", cex = 1.5)
points()
points(x1_x2, rep(0, length(x1_x2)))
par(mfrow=c(2,1))
nsamples <- 100000
# Sum of two Gaussians
x1 <- rnorm(nsamples, mean=-10, sd=1)
x2 <- rnorm(nsamples, mean=10, sd=1)
hist(x1+x2, breaks=100)
# Mixture of two Gaussians
z <- runif(nsamples)<0.5 # assume mixture coefficients are (0.5,0.5)
x1_x2 <- rnorm(nsamples,mean=ifelse(z,-10,10),sd=1)
hist(x1_x2,breaks=100)
points(x1_x2, rep(0, length(x1_x2)))
nsamples <- 1000
x1 <- rnorm(nsamples, mean=-10, sd=1)
x2 <- rnorm(nsamples, mean=10, sd=1)
hist(x1+x2, breaks=100)
# Mixture of two Gaussians
z <- runif(nsamples)<0.5 # assume mixture coefficients are (0.5,0.5)
x1_x2 <- rnorm(nsamples,mean=ifelse(z,-10,10),sd=1)
hist(x1_x2,breaks=100)
points(x1_x2, rep(0, length(x1_x2)))
?sample_pa
library(igraph)
?sample_pa
n <- 1000
p <- 1
g <- sample_pa(n, power=p, directed=FALSE, m=3)
la <- layout_with_fr(g)
plot(g,
layout = la,
vertex.label = "",
vertex.color = "black",
vertex.size = 1 + 0.7 * log( graph.strength(g), 3 ),
edge.width = 1.5,
asp=9/16,
margin=-0.15)
n <- 1000
p <- 1
g <- sample_pa(n, power=p, directed=FALSE)
la <- layout_with_fr(g)
plot(g,
layout = la,
vertex.label = "",
vertex.color = "black",
vertex.size = 1 + 0.7 * log( graph.strength(g), 3 ),
edge.width = 1.5,
asp=9/16,
margin=-0.15)
?barabasi.game
library(igraph)
set.seed(144)
g <- sample_pa(20, power=1, directed=TRUE)
plot(g)
get.edgelist(g)[,2]
# http://doi.org/10.1007/s11280-012-0162-8
nodes <- order(V(g)$time)[-1]
nodes <- order(V(g)$time)[-1]
V(g)
V(g)$time
V(g)$time <- 1:vcount(g)
V(g)$time
nodes <- order(V(g)$time)[-1]
nodes
parents <- get.edgelist(g)[,2][nodes+1]
parents
parents <- get.edgelist(g)[,2][nodes]
parents
nodes
get.edgelist(g)
nodes
parents
nodes
get.edgelist(g)[,2][nodes-1]
unlist(adjacent_vertices(g, nodes, mode = c("out")))
source('~/Documents/PhD/Thread_prediction/test_likelihood.r')
setwd("~/Documents/PhD/Thread_prediction")
source('~/Documents/PhD/Thread_prediction/test_likelihood.r')
library(utils)
Rprof(line.profiling=TRUE)
replicate(n=100, likelihood(c(1,1,1), g))
Rprof(NULL)
#summaryRprof(lines="both")
summaryRprof(lines="show")
g
library(microbenchmark)
install.packages("microbenchmark")
library(microbenchmark)
parents
nodes <- order(V(g)$time)[-1] # exclude root
parents <- get.edgelist(g)[,2][nodes-1]
#parents <- unlist(adjacent_vertices(g, nodes, mode = c("out"))) #bottleneck
parents
parents
microbenchmark(degrees.2(parents), degrees(parents))
degrees.2 <- function(parents){
# Computes the degrees of every node using the parents vector
# (slower in long parent vectors)
degrees <- rep(NA, length(parents)+1)
for(i in 1:length(degrees)){
degrees[i] <- sum(parents==i)
}
degrees
}
degrees <- function(parents){
# Computes the degrees of every node using the parents vector
degrees <- hist(parents, plot=F, breaks=c(seq(0,length(parents)+1)))$counts
degrees
}
depths <- function(parents){
# Computes the depths of every node using the parents vector
# depth: depth = parentdepth + 1
depths <- rep(0, length(parents)+1)
for(i in 1:length(parents)){
depths[i+1] <- depths[parents[i]] + 1
}
depths
}
likelihood <- function(params, g) {
# Computes total likelihood of a graph
# given the parameters
#
# Args:
#   params: vector with alpha, beta and lambda parameters
#   g: observed graph
#
# Returns:
#   loglikelihood of the graph
alpha <- params[1]
beta <- params[2]
lambda <- params[3]
parents <- parents.vector(g)
# likelihood of every parent choice
like <- 0
for (t in 2:length(parents)){
h <- depths(parents[1:(t-1)])
d <- degrees.2(parents[1:(t-1)])
#tau <- t - as.numeric(V(G)$date[i]) # how old is the post
probs <- (d+10e-5)^alpha * (h+10e-5)^beta #* taus^(-lambda)
probs <- probs/sum(probs)
like <- like + log(probs[parents[t]])
}
like
}
library(utils)
Rprof(line.profiling=TRUE)
replicate(n=100, likelihood(c(1,1,1), g))
Rprof(NULL)
#summaryRprof(lines="both")
summaryRprof(lines="show"
)
source('~/Documents/PhD/Thread_prediction/likelihood.r')
summaryRprof(lines="show")
parents
degrees.3 <- function(parents){
# Computes the degrees of every node using the parents vector
# (slower in long parent vectors)
degrees <- rep(NA, length(parents)+1)
for(i in 1:length(degrees)){
degrees[parents[i]] <- degrees[parents[i]] + 1
}
degrees
}
degrees.3(parents)
degrees.2(parents)
degrees(parents)
degrees.2(parents)
parents
[i]
parents[i]
degrees <- rep(NA, length(parents)+1)
for(i in 1:length(degrees)){
degrees[parents[i]] <- degrees[parents[i]] + 1
}
degrees
i
degrees <- rep(0, length(parents)+1)
for(i in 1:length(degrees)){
degrees[parents[i]] <- degrees[parents[i]] + 1
}
degrees
degrees.3 <- function(parents){
# Computes the degrees of every node using the parents vector
# (slower in long parent vectors)
degrees <- rep(0, length(parents)+1)
for(i in 1:length(degrees)){
degrees[parents[i]] <- degrees[parents[i]] + 1
}
degrees
}
degrees.2(parents)
degrees.3(parents)
microbenchmark(degrees.2(parents), degrees.3(parents))
source('~/Documents/PhD/Thread_prediction/likelihood.r')
library(utils)
Rprof(line.profiling=TRUE)
replicate(n=100, likelihood(c(1,1,1), g))
Rprof(NULL)
#summaryRprof(lines="both")
summaryRprof(lines="show")
depths.2 <- function(parents){
# Computes the depths of every node using the parents vector
# depth: depth = parentdepth + 1
depths <- rep(0, length(parents)+1)
for(i in 1:length(parents)){
depths[i+1] <- depths[parents[i]] + 1 # bottleneck
}
depths
}
degrees.3 <- function(parents){
# Computes the degrees of every node using the parents vector
degrees <- rep(0, length(parents)+1)
for(i in 1:length(degrees)){
degrees[parents[i]] <- degrees[parents[i]] + 1
}
degrees
}
microbenchmark(depths.2(parents), degrees.3(parents))
library(utils)
Rprof(line.profiling=TRUE)
replicate(n=100, likelihood(c(1,1,1), g))
Rprof(NULL)
#summaryRprof(lines="both")
summaryRprof(lines="show")
source('~/Documents/PhD/Thread_prediction/likelihood.r')
library(utils)
Rprof(line.profiling=TRUE)
replicate(n=100, likelihood(c(1,1,1), g))
Rprof(NULL)
#summaryRprof(lines="both")
summaryRprof(lines="show")
source('~/Documents/PhD/Thread_prediction/likelihood.r')
source('~/Documents/PhD/Thread_prediction/likelihood.r')
library(utils)
Rprof(line.profiling=TRUE)
replicate(n=100, likelihood(c(1,1,1), g))
Rprof(NULL)
#summaryRprof(lines="both")
summaryRprof(lines="show")
source('~/Documents/PhD/Thread_prediction/likelihood.r')
source('~/Documents/PhD/Thread_prediction/likelihood.r')
?apply
?lapply
?sapply
sapply(1:3, mean)
ff <- function(depths, parents, i){
depths[parents[i]]+1
}
ff(depths, parents, 1)
ff(depths, parents, 2)
depths
depths <- rep(0, length(parents)+1)
ff(depths, parents, 2)
ff(depths, parents, 3)
ff(depths, parents, 3)
ff(depths, parents, 3)
ff(depths, parents, 3)
ff(depths, parents, 3)
parents[2:4]
parents
parents[10:30]
depths[parents[10:30]]
depths <- rep(0, length(parents)+1) # include root node
for(i in 1:length(parents)){
depths[i+1] <- depths[parents[i]] + 1 # bottleneck
}
depths
depths
tables(parents)
table(parents)
source('~/Documents/PhD/Thread_prediction/test_likelihood.r')
source('~/Documents/PhD/Thread_prediction/test_likelihood.r')
source('~/Documents/PhD/Thread_prediction/test_likelihood.r')
source('~/Documents/PhD/Thread_prediction/test_likelihood.r')
source('~/Documents/PhD/Thread_prediction/test_likelihood.r')
interactive()
source('~/Documents/PhD/Thread_prediction/thread_generator.r')
source('~/Documents/PhD/Thread_prediction/thread_generator.r')
source('~/Documents/PhD/Thread_prediction/thread_generator.r')
source('~/Documents/PhD/Thread_prediction/test_likelihood.r')
source('~/Documents/PhD/Thread_prediction/thread_generator.r')
par(mfrow=c(4,3))
for (i in seq(0,1,length=12)){
alpha <- runif(1)
beta <- runif(1)
lambda <- runif(1)
g <- gen.thread(n=100, alpha=alpha, beta=beta, lambda=lambda)
# Plot
la <- layout_with_fr(g)
plot(as.undirected(g),
layout = la,
vertex.label = "",
vertex.color = "black",
vertex.size = 1.5 + 1.5 * log( graph.strength(g), 3 ),
edge.width = 1.5,
asp=9/16,
margin=-0.15)
title(bquote(alpha ~ '=' ~ .(round(alpha, digits=2)) ~ ';' ~
beta ~ '=' ~ .(round(beta, digits=2)) ~ ';' ~
lambda ~ '=' ~ .(round(lambda, digits=2))
)
)
}
list(10)
list()
ll <- list()
?ll
ll[2]
ll[2] <- "hola"
ll
ll[5] <- "hola"
ll
ll["5"] <- "hola"
ll
ll["5"] <- g
g
ll[5] <- g
lloutput <- vector("list", 20)[5] <- g
output <- vector("list", 20)
output
output[[1]] <- g
output
output[1] <- g
source('~/Documents/PhD/Thread_prediction/main.r')
source('~/Documents/PhD/Thread_prediction/main.r')
i
traceback()
trees
trees[[1:3]]
trees[1:3]
trees[1:50]
trees <- trees[1:50]
trees
g
length(g)
is.class()
class(g)
is.class(g)
class(g)
list()
qqq <- list()
qqq
class(qqq)
list(g)
aaa <- list(g)
aaa
trees
likelihood <- function(params, trees) {
# TODO: trees instead of g. should compute likelihood of all data.
# Computes total likelihood of a graph
# given the parameters
#
# Args:
#   params: vector with alpha, beta and lambda parameters
#   g: observed graph
#
# Returns:
#   loglikelihood of the graph
alpha <- params[1]
beta <- params[2]
lambda <- params[3]
# if only one tree, wrap it in a list
# so that next 'for' loop works fine
if(class(trees) == "igraph"){
trees <- list(g)
}
total.like <- 0
for(i in 1:length(trees)){
g <- trees[[i]]
parents <- parents.vector(g)
# likelihood of every parent choice
like <- 0
for (t in 2:length(parents)){
h <- depths(parents[1:(t-1)])
d <- degrees.3(parents[1:(t-1)])
#tau <- t - as.numeric(V(G)$date[i]) # how old is the post
probs <- (d+10e-5)^alpha * (h+10e-5)^beta #* taus^(-lambda)
probs <- probs/sum(probs)
like <- like + log(probs[parents[t]])
}
total.like <- total.like + like
}
total.like
}
trees
like <- likelihood(trees)
like <- likelihood(c(alpha, beta, lambda), trees)
like
alpha
beta
lambda
like <- likelihood(c(alpha, beta, lambda), trees)
like
like <- likelihood(c(3, 0, lambda), trees)
like
# Find MLE by grid search
alpha_ <- seq(0.5,5, by=0.1)
beta_ <- seq(30,100, by=3)
xy <- merge(alpha_, beta_)
like <- rep(NA, nrow(xy))
for(i in 1:nrow(xy)){
like[i] <- likelihood(c(xy[i,1], xy[i,2]), trees)
}
for(i in 1:nrow(xy)){
cat("\n", i, nrow(xy))
like[i] <- likelihood(c(xy[i,1], xy[i,2]), trees)
}
# plot grid
par(mfrow=c(1,1))
plot(xy[,1], xy[,2], cex=exp(like-max(like)), xlab=expression(alpha), ylab=expression(beta))
title("Grid search of MLE")
# plot grid
par(mfrow=c(1,1))
plot(xy[,1], xy[,2], cex=exp((like-max(like))/sd(abs(like))), xlab=expression(alpha), ylab=expression(beta))
title("Grid search of MLE")
alpha_mle <- xy[which.max(like),1]
beta_mle <- xy[which.max(like),2]
for (i in seq(0,1,length=9)){
lambda <- 0
g <- gen.thread(n=100, alpha=alpha_mle, beta=beta_mle, lambda=0)
# Plot
la <- layout_with_fr(g)
plot(as.undirected(g),
layout = la,
vertex.label = "",
vertex.color = "black",
vertex.size = 1.5 + 1.5 * log( graph.strength(g), 3 ),
edge.width = 1.5,
asp=9/16,
margin=-0.15)
title(bquote(alpha ~ '=' ~ .(round(alpha_mle, digits=2)) ~ ';' ~
beta ~ '=' ~ .(round(beta_mle, digits=2)) ~ ';' ~
lambda ~ '=' ~ .(round(lambda, digits=2))
)
)
title("Graphs with MLE parameters", outer=T, line=-1)
}
